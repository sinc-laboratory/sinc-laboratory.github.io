---
layout: default-publication
title: "Who Calls the Shots? Rethinking Few-Shot Learning for Audio"
collection: publications
permalink: /publications/2021-10-17-wang2021who
abstract: "While the field has seen notable advances in recent years, they have often focused on multi-class image classification. Audio, in contrast, is often multi-label due to overlapping sounds, resulting in unique properties such as polyphony and signal-to-noise ratios (SNR). This leads to unanswered questions concerning the impact such audio properties may have on few-shot learning system design, performance, and human-computer interaction, as it is typically up to the user to collect and provide inference-time support set examples. We address these questions through a series of experiments designed to elucidate the answers to these questions. We introduce two novel datasets, FSD-MIX-CLIPS and FSD-MIX-SED, whose programmatic generation allows us to explore these questions systematically. Our experiments lead to audio-specific insights on few-shot learning, some of which are at odds with recent findings in the image domain: there is no best one-size-fits-all model, method, and support set selection criterion. Rather, it depends on the expected application scenario."
date: 2021-10-17
venue: 'IEEE Workshop on Applications of Signal Processing to Audio and Acoustics'
venue_short: 'WASPAA'
paperurl: '/files/wang2021who.pdf'
image: '/assets/images/wang2021who_recap.png'
imagewidth: 75.0
presentation: '/files/wang2021who_presentation.pdf'
code: 'https://github.com/wangyu/rethink-audio-fsl'
codename: 'Code repository'
data: 'https://doi.org/10.5281/zenodo.5574135'
dataname: 'FSD-MIX-CLIPS'
categories: 
  - Environmental Machine Listening
citation: 'Wang, Y., Bryan, N., Salamon, J., Cartwright, M., Bello, J.P. Who calls the shots? Rethinking few-shot learning for audio. In <i>Proceedings of the IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)</i>, 2021. <b><i class="fas fa-fw fa-trophy" aria-hidden="true"></i> Special Best Paper Award</b>'
---