---
layout: default-publication
title: "MONYC: Music of New York City Dataset"
collection: publications
permalink: /publications/2021-10-24-fuentes2021monyc
abstract: "Music plays an important role in human cultures and constitutes an integral part of urban soundscapes. In order to make sense of these soundscapes, machine listening models should be able to detect and classify street music. Yet, the lack of well-curated resources for training and evaluating these models currently hinders their development. We present MONYC, an open dataset of 1.5k music clips as recorded by the sensors of the Sounds of New York City (SONYC) project. MONYC contains audio data and spatiotemporal metadata, i.e., coarse sensor location and timestamps. In addition, we provide multilabel genre tags from four annotators as well as four binary tags: whether the music is live or recorded; loud or quiet; single-instrument or multi-instrument; and whether non-musical sources are also present.  The originality of MONYC is that it reveals how music manifests itself in a real-world setting among social interactions in an urban context. We perform a detailed qualitative analysis of MONYC, show its spatiotemporal trends, and discuss the scope of research questions that it can answer in the future."
date: 2021-10-24
venue: 'Workshop on Detection and Classification of Acoustic Scenes and Events'
venue_short: 'DCASE'
paperurl: '/files/fuentes2021monyc.pdf'
presentation: '/files/fuentes2021monyc_presentation.pdf'
poster: '/files/fuentes2021monyc_poster.pdf'
data: 'https://magdalenafuentes.github.io/monyc/'
dataname: 'MONYC'
categories: 
  - Environmental Machine Listening
  - Music Information Retrieval
citation: 'Fuentes, M., Zhao, D., Lostanlen, V., Cartwright, M., Mydlarz, C., Bello, J.P. MONYC: Music of New York City Dataset. In <i>Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)</i>, 2021.'
---